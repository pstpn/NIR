\subsection{YOLO}

YOLO~---~сеть, предназначенная для идентификации и распознавания объектов на изображениях в реальном времени.
Такой подход к обнаружению объектов называется <<Вы смотрите только один раз>> (YOLO), что означает распознавание объектов сразу после первого прохода по изображению.
Метод YOLO рассматривает обнаружение объектов как задачу регрессии с пространственно разделенными ограничивающими рамками и соответствующими вероятностями классов, которые прогнозируются с помощью одной нейронной сети на основе полных изображений в ходе одной оценки.
YOLO быстра по своей конструкции и действительно работает в режиме реального времени, сохраняя большую точность~\cite{base, all}.

Базовая модель YOLO также называется YOLO версии 1 (YOLOv1).
Она решает задачу обнаружения объектов на изображении как задачу регрессии.
Одна сверточная сеть одновременно предсказывает множество ограничивающих рамок и вероятности классов для этих рамок.
YOLOv1 разбивает входное изображение на сетку $S \times S$.
Если центр объекта попадает в ячейку сетки, эта ячейка отвечает за обнаружение этого объекта.
Каждая ячейка сетки предсказывает $B$ ограничивающих рамок, показатели достоверности для этих рамок и вероятности класса $C$ для сетки.
Эти прогнозы закодированы в виде тензора $S \times S \times (B \times 5 + C)$.
В процессе тестирования YOLOv1 умножает условные вероятности классов и прогнозы достоверности отдельных блоков, которые дают оценки для каждого блока, относящиеся к конкретному классу по формуле~\ref{eq1}~\cite{all}.
\begin{equation}
	\label{eq1}
	Pr(Class_{i} | Object) \times Pr(Object) \times IOU^{truth}_{pred} = Pr(Class_{i}) \times IOU^{truth}_{pred}
\end{equation}

Оценки отражают вероятность появления этого класса в поле и схожесть поля с объектом.
Каждое ограничивающее поле состоит из 5 прогнозов: $x$, $y$, $w$, $h$ и достоверности.
Координаты $(x, y)$ представляют центр прямоугольника относительно границ ячейки сетки.
Ширина $w$ и высота $h$ рассчитываются относительно всего изображения.
Именно поэтому YOLOv1 использует выражение $B \times 5$ для вычисления тензора.
Сеть YOLOv1 состоит из 24 сверточных слоев, за которыми следуют 2 полностью соединенных слоя.
Вместо начальных модулей, используемых в GoogLeNet, YOLOv1 использует слой сокращения 1 $\times$ 1, за которым следуют сверточные слои 3 $\times$ 3.
В Pascal VOC2007 YOLOv1 обрабатывает изображения со скоростью 45 кадров в секунду (FPS), что в 2~---~9 раз быстрее, чем у Faster R-CNN.
В частности, Fast YOLO, быстрая версия YOLO, разработанная для расширения возможностей быстрого обнаружения объектов, достигает 155 кадров в секунду (FPS)~\cite{base, all}.

Архитектура YOLOv1 CNN представлена на рисунке~\ref{img:yolov1}.
\includeimage
	{yolov1}
	{f}
	{H}
	{1\textwidth}
	{Архитектура YOLOv1 CNN}

YOLO версии 2 (YOLOv2)~---~значительно улучшенная модель YOLO, которая сохраняет преимущество в скорости и пытается повысить качество распознавания по сравнению с YOLOv1.
Используя новый многомасштабный метод обучения, одна и та же модель YOLOv2 может работать в разных размерностях, предлагая простой компромисс между скоростью и качеством.
Алгоритм лучше справляется с небольшими объектами и реагирует быстрее, чем ранее доступные версии.
Одноступенчатая архитектура предполагает наличие только одной нейронной сети для прогнозирования ограничивающей рамки и вероятности категории.
YOLOv2 имеет множество улучшений по сравнению со своими предшественниками и другими алгоритмами.
В первую очередь, YOLOv2 использует Darknet-19 с 19 сверточными слоями и 5 слоями Max-Pooling и вводит якорные рамки (anchor boxes), что позволяет лучше адаптироваться к объектам разных размеров.
Вместо абсолютных значений координаты предсказываются как смещения относительно якорных рамок, для каждой из которых модель предсказывает параметры координат, уверенность и вероятности классов~\cite{yolochina, yolobase}.

Архитектура YOLOv2 CNN представлена на рисунке~\ref{img:yolov2}.
\includeimage
	{yolov2}
	{f}
	{H}
	{1\textwidth}
	{Архитектура YOLOv2 CNN}

\subsection{R-CNN}

\subsection{Fast R-CNN}

\subsection{Faster R-CNN}